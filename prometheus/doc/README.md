# Goal

Monitoring all compoments in pai, provide insight on detectiving system/hardware failuring and
analysing jobs performance.

# Architecture

![Architecture](architecture.png)

We have three parts consisting Pai's monitoring system: `watchdog`, `exporter` and
[`prometheus`](https://prometheus.io/).

`prometheus` is an open source solution for metrics collecting, storage and querying.

`watchdog` is a single instance pod responsible for monitoring k8s/nodes/pods health, it will first
list all pod/node from kubernetes api server and check their status and log their status, this is very
helpful for debugging.

`exporter` are those pods running in the lefe side of node, they are responsible for collecting
metrics from jobs/nodes/gpus. There are two containers running inside `exporter` pod: `gpu_exporter`
and [`node_exporter`](https://github.com/prometheus/node_exporter): `gpu_exporter` exposes job/gpu
metrics to volume mounted in `/datastorage/prometheus`.

Metrics generated by `watchdog` and `gpu_exporter` are collected by `node_exporter` container running
inside `exporter` pod. Those metrics are scraped by `node_exporter` container. `node_exporter` also
expose node metricss like node cpu/memory/disk usage.

# Metrics collected

Exporter's metrics are listed [here](./exporter-metrics.md).

More metrics are listed [here](./watchdog-metrics.md).

# Metrics used

The most important usage of metrics is for alerting, checkout [rule directory](../prometheus-alert)
to see metrics we already used for alerting.

Other pai component also used some metrics for display, they are:

<table>
<tr>
    <td>Component</td>
    <td>Metric used</td>
</tr>
<tr>
    <td>Grafana</td>
    <td>
        <ul>
            <li>node_uname_info</li>
            <li>nvidiasmi_utilization_gpu</li>
            <li>nvidiasmi_utilization_memory</li>
            <li>nvidiasmi_attached_gpus</li>
            <li>node_cpu_seconds_total</li>
            <li>node_memory_MemTotal_bytes</li>
            <li>node_memory_MemFree_bytes</li>
            <li>node_memory_Buffers_bytes</li>
            <li>node_memory_Cached_bytes</li>
            <li>node_network_receive_bytes_total</li>
            <li>node_network_transmit_bytes_total</li>
            <li>node_disk_read_bytes_total</li>
            <li>node_disk_written_bytes_total</li>
            <li>container_CPUPerc</li>
            <li>container_MemUsage</li>
            <li>container_NetIn</li>
            <li>container_NetOut</li>
            <li>container_BlockIn</li>
            <li>container_BlockOut</li>
            <li>container_GPUPerc</li>
            <li>container_GPUMemPerc</li>
        </ul>
    </td>
</tr>
<tr>
    <td>WebPortal</td>
    <td>
        <ul>
            <li>node_cpu_seconds_total</li>
            <li>node_memory_MemTotal_bytes</li>
            <li>node_memory_MemFree_bytes</li>
            <li>node_memory_Buffers_bytes</li>
            <li>node_memory_Cached_bytes</li>
            <li>node_disk_read_bytes_total</li>
            <li>node_disk_written_bytes_total</li>
            <li>node_network_receive_bytes_total</li>
            <li>node_disk_written_bytes_total</li>
        </ul>
    </td>
</tr>
</table>

# Build

Build image by using `paictl.py`:
```
./paictl.py image build -p ~/pai-config/ -n gpu-exporter
./paictl.py image build -p ~/pai-config/ -n watchdog
```

push to registry for deploying:

```
./paictl.py image push -p ~/pai-config/ -n gpu-exporter
./paictl.py image push -p ~/pai-config/ -n watchdog
```

# Deployment

start by:

```
./paictl.py service start -p ~/pai-config/ -n prometheus
```

stop by:
```
./paictl.py service stop -p ~/pai-config/ -n prometheus
```

stop and clean data by:
```
./paictl.py service delete -p ~/pai-config/ -n prometheus
```
